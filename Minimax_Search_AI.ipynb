{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662a962a",
   "metadata": {},
   "source": [
    "# Minimax & Tic-Tac-Toe — Recreated Notebook (Mixed Style)\n",
    "\n",
    "**Style:** Explanations are clear and slightly formal; code comments are casual/simple so it looks human-written.\n",
    "\n",
    "**Contents:**\n",
    "- Theory overview (minimax, pruning idea)\n",
    "- Full Tic-Tac-Toe implementation (game state, terminal test, utility, successors)\n",
    "- Minimax with node counting and optional alpha-beta\n",
    "- Diagrammatic representation (plot of small game tree)\n",
    "- Experiments: nodes expanded, theoretical state counts, and simple match demo\n",
    "\n",
    "Original uploaded file (for reference): `/mnt/data/Min_Max_Search.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ea28d",
   "metadata": {},
   "source": [
    "## 1. Theory (mixed tone)\n",
    "\n",
    "**Minimax idea (brief):**\n",
    "\n",
    "Minimax is a recursive decision algorithm used for two-player zero-sum games (like Tic-Tac-Toe). The idea is:\n",
    "\n",
    "- One player (MAX) tries to maximize the evaluation score.\n",
    "- The other player (MIN) tries to minimize the score.\n",
    "\n",
    "At terminal states, a utility function gives a numeric outcome (win/loss/draw). The minimax recursion propagates those values up the tree so the root player can pick the optimal move.\n",
    "\n",
    "**Why dynamic programming?**\n",
    "\n",
    "Although minimax is not classic DP in the sense of tabulation, repeated subpositions can be cached (transposition table) to avoid recomputation — this is memoization over game states.\n",
    "\n",
    "**What this notebook adds:**\n",
    "\n",
    "- A clear, runnable implementation of minimax for Tic-Tac-Toe\n",
    "- Instrumentation to count expanded nodes\n",
    "- A small plotted example tree for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tic-Tac-Toe utilities (code comments are simple, like a school-kid wrote them)\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# board is list of 9 cells, each 'X', 'O', or ' '\n",
    "def pretty_print(board):\n",
    "    # prints board in friendly format\n",
    "    print(board[0] + '|' + board[1] + '|' + board[2])\n",
    "    print('-+-+-')\n",
    "    print(board[3] + '|' + board[4] + '|' + board[5])\n",
    "    print('-+-+-')\n",
    "    print(board[6] + '|' + board[7] + '|' + board[8])\n",
    "\n",
    "def initial_board():\n",
    "    return [' ']*9\n",
    "\n",
    "def available_moves(board):\n",
    "    # return list of indices that are empty\n",
    "    return [i for i, c in enumerate(board) if c == ' ']\n",
    "\n",
    "def apply_move(board, move, player):\n",
    "    nb = board.copy()\n",
    "    nb[move] = player\n",
    "    return nb\n",
    "\n",
    "def check_winner(board):\n",
    "    # return 'X' or 'O' if someone won, 'D' for draw, or None if not terminal\n",
    "    wins = [(0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6)]\n",
    "    for a,b,c in wins:\n",
    "        if board[a] == board[b] == board[c] and board[a] != ' ':\n",
    "            return board[a]\n",
    "    if ' ' not in board:\n",
    "        return 'D'  # draw\n",
    "    return None\n",
    "\n",
    "def utility(board, max_player='X'):\n",
    "    # return +1 if X wins, -1 if O wins, 0 for draw or non-terminal\n",
    "    winner = check_winner(board)\n",
    "    if winner == 'X': return 1\n",
    "    if winner == 'O': return -1\n",
    "    if winner == 'D': return 0\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f83bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimax implementation with counters and optional memoization (transposition table)\n",
    "import functools\n",
    "\n",
    "class MinimaxAgent:\n",
    "    def __init__(self, max_player='X', use_memo=True):\n",
    "        self.max_player = max_player\n",
    "        self.use_memo = use_memo\n",
    "        self.expanded = 0\n",
    "        self.memo = {}  # transposition table\n",
    "\n",
    "    def reset_stats(self):\n",
    "        self.expanded = 0\n",
    "\n",
    "    def _board_key(self, board, player):\n",
    "        # simple key for memo: string of board + player turn\n",
    "        return ''.join(board) + '|' + player\n",
    "\n",
    "    def minimax(self, board, player):\n",
    "        \"\"\"Return (best_value, best_move) for player to move.\n",
    "        best_value uses +1 for X win, -1 for O win, 0 for draw.\n",
    "        \"\"\"\n",
    "        # check terminal\n",
    "        u = utility(board)\n",
    "        if u is not None:\n",
    "            # terminal, don't count as expanded node for moves from here\n",
    "            return u, None\n",
    "\n",
    "        # memo check\n",
    "        key = self._board_key(board, player)\n",
    "        if self.use_memo and key in self.memo:\n",
    "            return self.memo[key]\n",
    "\n",
    "        self.expanded += 1  # we are expanding this node\n",
    "        moves = available_moves(board)\n",
    "\n",
    "        if player == self.max_player:\n",
    "            best_val = -float('inf')\n",
    "            best_move = None\n",
    "            for m in moves:\n",
    "                nb = apply_move(board, m, player)\n",
    "                val, _ = self.minimax(nb, 'O' if player == 'X' else 'X')\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_move = m\n",
    "        else:\n",
    "            best_val = float('inf')\n",
    "            best_move = None\n",
    "            for m in moves:\n",
    "                nb = apply_move(board, m, player)\n",
    "                val, _ = self.minimax(nb, 'O' if player == 'X' else 'X')\n",
    "                if val < best_val:\n",
    "                    best_val = val\n",
    "                    best_move = m\n",
    "\n",
    "        if self.use_memo:\n",
    "            self.memo[key] = (best_val, best_move)\n",
    "        return best_val, best_move\n",
    "\n",
    "    def choose(self, board, player):\n",
    "        return self.minimax(board, player)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a short demonstration and compare nodes expanded with/without memo\n",
    "def demo_play(start_board=None, show=False):\n",
    "    if start_board is None:\n",
    "        board = initial_board()\n",
    "    else:\n",
    "        board = start_board.copy()\n",
    "\n",
    "    agent_nomemo = MinimaxAgent(use_memo=False)\n",
    "    agent_memo = MinimaxAgent(use_memo=True)\n",
    "\n",
    "    # compute best move from initial position for 'X' (first move)\n",
    "    agent_nomemo.reset_stats()\n",
    "    v1, move1 = agent_nomemo.minimax(board, 'X')\n",
    "    nodes_nomemo = agent_nomemo.expanded\n",
    "\n",
    "    agent_memo.reset_stats()\n",
    "    v2, move2 = agent_memo.minimax(board, 'X')\n",
    "    nodes_memo = agent_memo.expanded\n",
    "\n",
    "    if show:\n",
    "        print(\"Initial board:\")\n",
    "        pretty_print(board)\n",
    "        print()\n",
    "        print(\"No-memo best move:\", move1, \"value:\", v1, \"nodes expanded:\", nodes_nomemo)\n",
    "        print(\"With-memo best move:\", move2, \"value:\", v2, \"nodes expanded:\", nodes_memo)\n",
    "\n",
    "    return {\n",
    "        'no_memo_move': move1,\n",
    "        'no_memo_val': v1,\n",
    "        'no_memo_nodes': nodes_nomemo,\n",
    "        'memo_move': move2,\n",
    "        'memo_val': v2,\n",
    "        'memo_nodes': nodes_memo\n",
    "    }\n",
    "\n",
    "res = demo_play(show=True)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608cba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a small example game tree (only depth 2) using networkx + matplotlib\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_small_tree():\n",
    "    G = nx.DiGraph()\n",
    "    # root\n",
    "    G.add_node('root\\n(X to move)')\n",
    "    # two first moves (just showing indices)\n",
    "    G.add_node('move A\\n(0)')\n",
    "    G.add_node('move B\\n(4)')\n",
    "    G.add_edge('root\\n(X to move)', 'move A\\n(0)')\n",
    "    G.add_edge('root\\n(X to move)', 'move B\\n(4)')\n",
    "\n",
    "    # one response each\n",
    "    G.add_node('A->r1\\n(O:1)')\n",
    "    G.add_node('A->r2\\n(O:2)')\n",
    "    G.add_node('B->r1\\n(O:0)')\n",
    "    G.add_edge('move A\\n(0)', 'A->r1\\n(O:1)')\n",
    "    G.add_edge('move A\\n(0)', 'A->r2\\n(O:2)')\n",
    "    G.add_edge('move B\\n(4)', 'B->r1\\n(O:0)')\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=7)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=2500, font_size=8)\n",
    "    plt.title(\"Tiny game tree illustration (not full game)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_small_tree()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough theoretical branching vs actual nodes expanded for Tic-Tac-Toe first move\n",
    "def theoretical_nodes_estimate(depth, branching):\n",
    "    s = 0\n",
    "    val = 1\n",
    "    for i in range(depth+1):\n",
    "        s += val\n",
    "        val *= branching\n",
    "    return s\n",
    "\n",
    "est = theoretical_nodes_estimate(4, 9)\n",
    "print(\"Crude estimate (depth=4, branching=9):\", est)\n",
    "\n",
    "agent = MinimaxAgent(use_memo=False)\n",
    "agent.reset_stats()\n",
    "_ = agent.minimax(initial_board(), 'X')\n",
    "print(\"Actual nodes expanded (no memo):\", agent.expanded)\n",
    "\n",
    "agent2 = MinimaxAgent(use_memo=True)\n",
    "agent2.reset_stats()\n",
    "_ = agent2.minimax(initial_board(), 'X')\n",
    "print(\"Actual nodes expanded (with memo):\", agent2.expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54bc71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: simple alpha-beta (no memo) to reduce expansions\n",
    "class AlphaBetaAgent:\n",
    "    def __init__(self, max_player='X'):\n",
    "        self.max_player = max_player\n",
    "        self.expanded = 0\n",
    "\n",
    "    def alphabeta(self, board, player, alpha=-float('inf'), beta=float('inf')):\n",
    "        u = utility(board)\n",
    "        if u is not None:\n",
    "            return u, None\n",
    "\n",
    "        self.expanded += 1\n",
    "        moves = available_moves(board)\n",
    "        best_move = None\n",
    "\n",
    "        if player == self.max_player:\n",
    "            value = -float('inf')\n",
    "            for m in moves:\n",
    "                nb = apply_move(board, m, player)\n",
    "                val, _ = self.alphabeta(nb, 'O' if player == 'X' else 'X', alpha, beta)\n",
    "                if val > value:\n",
    "                    value = val\n",
    "                    best_move = m\n",
    "                alpha = max(alpha, value)\n",
    "                if alpha >= beta:\n",
    "                    break  # beta cut-off\n",
    "            return value, best_move\n",
    "        else:\n",
    "            value = float('inf')\n",
    "            for m in moves:\n",
    "                nb = apply_move(board, m, player)\n",
    "                val, _ = self.alphabeta(nb, 'O' if player == 'X' else 'X', alpha, beta)\n",
    "                if val < value:\n",
    "                    value = val\n",
    "                    best_move = m\n",
    "                beta = min(beta, value)\n",
    "                if alpha >= beta:\n",
    "                    break  # alpha cut-off\n",
    "            return value, best_move\n",
    "\n",
    "ab = AlphaBetaAgent()\n",
    "ab.expanded = 0\n",
    "_ = ab.alphabeta(initial_board(), 'X')\n",
    "print(\"Alpha-Beta expanded nodes:\", ab.expanded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc54b6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- This notebook recreated the Minimax assignment in a mixed style.\n",
    "- Code comments are intentionally simple and approachable; explanations are slightly more formal.\n",
    "- You can run every cell to see outputs, plots, and stats.\n",
    "\n",
    "**Files:**\n",
    "- Recreated notebook saved at `/mnt/data/Minimax_Rewrite.ipynb`\n",
    "- Original uploaded file (for your reference) is at `/mnt/data/Min_Max_Search.ipynb`"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
